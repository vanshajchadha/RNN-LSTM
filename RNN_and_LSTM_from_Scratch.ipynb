{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKqlmz62skBy"
   },
   "source": [
    "# RNN and LSTM Forward Propagation Implementation from Scratch (using NumPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BDb_eBAiJvDX"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "s-QvmfQZK9nq"
   },
   "outputs": [],
   "source": [
    "# Activation functions and their derivatives\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(sigmoid_x):\n",
    "  return sigmoid_x * (1 - sigmoid_x)\n",
    "\n",
    "def tanh(x):\n",
    "  \"\"\"\n",
    "    tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n",
    "  \"\"\"\n",
    "  return np.tanh(x)\n",
    "\n",
    "def tanh_deriv(tanh_x):\n",
    "  \"\"\"\n",
    "    der(tanh(x)) = 1 - tanh^2(x)\n",
    "  \"\"\"\n",
    "  return 1 - tanh_x ** 2\n",
    "\n",
    "def softmax(x):\n",
    "  \"\"\"\n",
    "  Subtracting the max for numerical stability and to prevent overflows.\n",
    "  The result of the softmax doesn't change.\n",
    "  np.exp(np.max(x)) gets cancelled out from numerator and denominator.\n",
    "  \"\"\"\n",
    "  e_x = np.exp(x - np.max(x))\n",
    "  return e_x / e_x.sum(axis = 0)\n",
    "\n",
    "def softmax_deriv(softmax_x):\n",
    "  \"\"\"\n",
    "  Softmax derivative naive implementation\n",
    "  \"\"\"\n",
    "  n,m = softmax_x.shape\n",
    "  derivative = np.zeros((n,n,m))\n",
    "  for col in range(m):\n",
    "    deriv = derivative[:,:,col]\n",
    "    for i in range(len(deriv)):\n",
    "      for j in range(len(deriv[i])):\n",
    "        if i == j:\n",
    "          deriv[i,j] = softmax_x[i,col] * (1 - softmax_x[j,col])\n",
    "        else:\n",
    "           deriv[i,j] = - softmax_x[i,col] * softmax_x[j,col]\n",
    "  return derivative\n",
    "\n",
    "def softmax_deriv_vectorized(softmax_x):\n",
    "  \"\"\"\n",
    "  Softmax derivative vectorized implementation\n",
    "  \"\"\"\n",
    "  n,m = softmax_x.shape\n",
    "  derivative = np.zeros((n,n,m))\n",
    "  for col in range(m):\n",
    "    vec = softmax_x[:,col].reshape(-1,1)\n",
    "    derivative[:,:,col] = np.diagflat(vec) - np.dot(vec, vec.T)\n",
    "  return derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0QTrpm73UZ0d"
   },
   "outputs": [],
   "source": [
    "# Loss function\n",
    "\n",
    "def cross_entropy_loss_t(y_t, ypred_t):\n",
    "  \"\"\"\n",
    "  Loss function across a single time step for a mini-batch size of \"m\"\n",
    "  \"\"\"\n",
    "  return np.sum(-y_t * np.log(ypred_t), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PoopWsEJSleN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax Naive:  [[[-1.45959458  0.13345295  0.18584352]\n",
      "  [-0.4892793   0.06574974  1.36109001]\n",
      "  [-0.02410591 -0.12634143 -1.24366813]]\n",
      "\n",
      " [[-0.4892793   0.06574974  1.36109001]\n",
      "  [-0.97303172 -0.58637766 -5.07159149]\n",
      "  [-0.01808754  0.33020073  2.98308384]]\n",
      "\n",
      " [[-0.02410591 -0.12634143 -1.24366813]\n",
      "  [-0.01808754  0.33020073  2.98308384]\n",
      "  [-0.03074311  0.1620561  -1.07475274]]] \n",
      "\n",
      "Softmax Vectorized:  [[[-1.45959458  0.13345295  0.18584352]\n",
      "  [-0.4892793   0.06574974  1.36109001]\n",
      "  [-0.02410591 -0.12634143 -1.24366813]]\n",
      "\n",
      " [[-0.4892793   0.06574974  1.36109001]\n",
      "  [-0.97303172 -0.58637766 -5.07159149]\n",
      "  [-0.01808754  0.33020073  2.98308384]]\n",
      "\n",
      " [[-0.02410591 -0.12634143 -1.24366813]\n",
      "  [-0.01808754  0.33020073  2.98308384]\n",
      "  [-0.03074311  0.1620561  -1.07475274]]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(3,3)\n",
    "\n",
    "sn = softmax_deriv(x)\n",
    "sv = softmax_deriv_vectorized(x)\n",
    "\n",
    "assert np.allclose(softmax_deriv(x), softmax_deriv_vectorized(x))\n",
    "\n",
    "print(\"Softmax Naive: \", sn, '\\n')\n",
    "print(\"Softmax Vectorized: \", sv, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "26fsx_RKMN3s"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "rnn_cell_forward is forward pass across a single time step\n",
    "\"\"\"\n",
    "def rnn_cell_forward(xt, a_prev, parameters):\n",
    "  \"\"\"\n",
    "  Shapes of each input, parameter and output:\n",
    "  Inputs:\n",
    "    xt -> (nx, m)\n",
    "    a_prev / a_next -> (nh, m)\n",
    "  Parameters:\n",
    "    Wax -> (nh, nx)\n",
    "    Waa -> (nh, nh)\n",
    "    Wya -> (ny, nh)\n",
    "    ba -> (nh, 1)\n",
    "    by -> (ny, 1)\n",
    "  Output:\n",
    "    yt_pred -> (ny, m)\n",
    "  \"\"\"\n",
    "\n",
    "  Wax = parameters['Wax']     # You can choose to concatenate the weights \"Waa\" and \"Wax\" into one single matrix, this is for better understanding\n",
    "  Waa = parameters['Waa']\n",
    "  Wya = parameters['Wya']\n",
    "  ba = parameters['ba']\n",
    "  by = parameters['by']\n",
    "\n",
    "  a_next = tanh(np.dot(Wax, xt) + np.dot(Waa, a_prev) + ba)\n",
    "\n",
    "  yt_pred = softmax(np.dot(Wya, a_next) + by)\n",
    "\n",
    "  cache = {'a_prev': a_prev, 'a_next': a_next, 'xt': xt, 'param': parameters}\n",
    "\n",
    "  return a_next, yt_pred, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Y4nOsqDGRKxr"
   },
   "outputs": [],
   "source": [
    "nx = 10 # dimension of a single vector in xt required for a word embedding i.e. vocabulary size\n",
    "nh = 8  # dimension of a single vector in a_prev encoding the hidden state information corresponding to the relevant input\n",
    "ny = 4  # dimension of a single vector in yt_pred representing the number of output classes per input vector\n",
    "m = 10  # mini-batch size\n",
    "np.random.seed(1)\n",
    "parameters = {'Wax': np.random.randn(nh, nx), 'Waa': np.random.randn(nh, nh), 'Wya': np.random.randn(ny, nh),\n",
    "              'ba': np.random.randn(nh, 1), 'by': np.random.randn(ny, 1)}\n",
    "xt = np.random.randn(nx, m)\n",
    "a_prev = np.random.randn(nh, m)\n",
    "a_next_result, yt_pred_result, cache = rnn_cell_forward(xt, a_prev, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IxoQdcr5WKrn",
    "outputId": "b7448416-2507-4864-a657-e338adbe29d5"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Forward propagation by performing rnn_cell_forward Tx times\n",
    "\"\"\"\n",
    "import random\n",
    "\n",
    "Tx = 5\n",
    "Ty = 5\n",
    "\n",
    "x = np.random.randn(Tx, nx, m)\n",
    "y_pred = np.zeros((Ty, ny, m))\n",
    "y = np.zeros((Ty, ny, m))\n",
    "idxs = [random.randint(0, ny-1) for _ in range(m)]\n",
    "for i in range(Ty):\n",
    "  for j in range(m):\n",
    "    y[i, idxs[j], j] = 1\n",
    "  random.shuffle(idxs)\n",
    "a = np.zeros((Tx, nh, m))\n",
    "\n",
    "parameters = {'Wax': np.random.randn(nh, nx), 'Waa': np.random.randn(nh, nh), 'Wya': np.random.randn(ny, nh),\n",
    "              'ba': np.random.randn(nh, 1), 'by': np.random.randn(ny, 1)}\n",
    "\n",
    "caches = []\n",
    "\n",
    "a_prev = np.zeros((nh, m))\n",
    "\n",
    "for t in range(Tx):\n",
    "  a[t,:,:], y[t,:,:], cache = rnn_cell_forward(x[t,:,:], a_prev, parameters)\n",
    "  a_prev = a[t,:,:]\n",
    "  caches.append(cache)\n",
    "\n",
    "# cross_entropy_loss_t(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VjnB_G3xxs1F"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "lstm_cell_forward is forward pass across a single time step\n",
    "\"\"\"\n",
    "def lstm_cell_forward(xt, a_prev, c_prev, parameters):\n",
    "  \"\"\"\n",
    "  Shapes of each input, parameter and output:\n",
    "\n",
    "  Let nd = nh + nx,\n",
    "\n",
    "  Inputs:\n",
    "    xt -> (nx, m)\n",
    "    a_prev / a_next -> (nh, m)\n",
    "    c_prev / c_next -> (nh, m)\n",
    "    a_xt_concat -> (nd, m)\n",
    "  Parameters:\n",
    "    Wc -> (nh, nd)\n",
    "    bc -> (nh, 1)\n",
    "    Wf -> (nh, nd)\n",
    "    bf -> (nh, 1)\n",
    "    Wi -> (nh, nd)\n",
    "    bi -> (nh, 1)\n",
    "    Wo -> (nh, nd)\n",
    "    bo -> (nh, 1)\n",
    "    Wy -> (ny, nh)\n",
    "    by -> (ny, 1)\n",
    "  Output:\n",
    "    yt_pred -> (ny, m)\n",
    "  \"\"\"\n",
    "  Wf = parameters['Wf']\n",
    "  bf = parameters['bf']\n",
    "  Wi = parameters['Wi']\n",
    "  bi = parameters['bi']\n",
    "  Wo = parameters['Wo']\n",
    "  bo = parameters['bo']\n",
    "  Wc = parameters['Wc']\n",
    "  bc = parameters['bc']\n",
    "  Wy = parameters['Wy']\n",
    "  by = parameters['by']\n",
    "\n",
    "  assert a_prev.shape[1] == xt.shape[1]\n",
    "\n",
    "  # Using this approach to reduce the number of parameters\n",
    "  a_xt_concat = np.concatenate([a_prev, xt], axis = 0)\n",
    "\n",
    "  # Shape of following gate results = (nh, m)\n",
    "  ft = sigmoid(np.dot(Wf, a_xt_concat) + bf)\n",
    "  it = sigmoid(np.dot(Wi, a_xt_concat) + bi)\n",
    "  ot = sigmoid(np.dot(Wo, a_xt_concat) + bo)\n",
    "\n",
    "  c_cand = tanh(np.dot(Wc, a_xt_concat) + bc)\n",
    "\n",
    "  assert ft.shape == c_prev.shape\n",
    "  assert it.shape == c_cand.shape\n",
    "  assert c_prev.shape == c_cand.shape\n",
    "\n",
    "  c_next = ft * c_prev + it * c_cand\n",
    "\n",
    "  assert ot.shape == c_next.shape\n",
    "\n",
    "  a_next = ot * tanh(c_next)\n",
    "\n",
    "  yt_pred = softmax(np.dot(Wy, a_next) + by)\n",
    "\n",
    "  cache = {'a_prev': a_prev, 'a_next': a_next, 'c_prev': c_prev, 'c_next': c_next, 'xt': xt, 'param': parameters}\n",
    "\n",
    "  return a_next, c_next, yt_pred, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "x9lbOm6t58Ob"
   },
   "outputs": [],
   "source": [
    "nx = 10 # dimension of a single vector in xt required for a word embedding i.e. vocabulary size\n",
    "nh = 8  # dimension of a single vector in a_prev/c_prev encoding the hidden state information corresponding to the relevant input\n",
    "ny = 4  # dimension of a single vector in yt_pred representing the number of output classes per input vector\n",
    "m = 10  # mini-batch size\n",
    "np.random.seed(1)\n",
    "xt = np.random.randn(nx, m)\n",
    "a_prev = np.random.randn(nh, m)\n",
    "c_prev = np.random.randn(nh, m)\n",
    "parameters = {'Wf': np.random.randn(nh, nh+nx), 'bf': np.random.randn(nh, 1), 'Wi': np.random.randn(nh, nh+nx),\n",
    "              'bi': np.random.randn(nh, 1), 'Wo': np.random.randn(nh, nh+nx), 'bo': np.random.randn(nh, 1),\n",
    "              'Wc': np.random.randn(nh, nh+nx), 'bc': np.random.randn(nh, 1), 'Wy': np.random.randn(ny, nh), 'by': np.random.randn(ny, 1)}\n",
    "\n",
    "a_next_result, c_next_result, yt_pred_result, cache = lstm_cell_forward(xt, a_prev, c_prev, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "KD49oJpu74IE"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Forward propagation by performing lstm_cell_forward Tx times\n",
    "\"\"\"\n",
    "Tx = 10\n",
    "Ty = 10\n",
    "\n",
    "x = np.random.randn(Tx, nx, m)\n",
    "y = np.zeros((Ty, ny, m))\n",
    "a = np.zeros((Tx, nh, m))\n",
    "c = np.zeros((Tx, nh, m))\n",
    "\n",
    "parameters = {'Wf': np.random.randn(nh, nh+nx), 'bf': np.random.randn(nh, 1), 'Wi': np.random.randn(nh, nh+nx),\n",
    "              'bi': np.random.randn(nh, 1), 'Wo': np.random.randn(nh, nh+nx), 'bo': np.random.randn(nh, 1),\n",
    "              'Wc': np.random.randn(nh, nh+nx), 'bc': np.random.randn(nh, 1), 'Wy': np.random.randn(ny, nh), 'by': np.random.randn(ny, 1)}\n",
    "\n",
    "caches = []\n",
    "\n",
    "a_prev = np.zeros((nh, m))\n",
    "c_prev = np.zeros((nh, m))\n",
    "\n",
    "for t in range(Tx):\n",
    "  a[t,:,:], c[t,:,:], y[t,:,:], cache = lstm_cell_forward(x[t,:,:], a_prev, c_prev, parameters)\n",
    "  a_prev = a[t,:,:]\n",
    "  c_prev = c[t,:,:]\n",
    "  caches.append(cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gxKHfMdHA-0u"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
